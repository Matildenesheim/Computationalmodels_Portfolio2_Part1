---
title: "Computational Modeling - Week 4 - Assignment 2 - Part 1"
author: "Riccardo Fusaroli"
date: "2/19/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## In this assignment we learn how to assess rates from a binomial distribution, using the case of assessing your teachers' knowledge of CogSci

### First part

You want to assess your teachers' knowledge of cognitive science. "These guys are a bunch of drama(turgist) queens, mindless philosophers, chattering communication people and Russian spies. Do they really know CogSci?", you think.

To keep things simple (your teachers should not be faced with too complicated things):
- You created a pool of equally challenging questions on CogSci
- Each question can be answered correctly or not (we don't allow partially correct answers, to make our life simpler).
- Knowledge of CogSci can be measured on a scale from 0 (negative knowledge, all answers wrong) through 0.5 (random chance) to 1 (awesome CogSci superpowers)

This is the data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

Questions:

1. What's Riccardo's estimated knowledge of CogSci? What is the probability he knows more than chance (0.5) [try figuring this out. if you can't peek into chapters 3.1 and 3.2 and/or the slides]?
- First implement a grid approximation (hint check paragraph 2.4.1!) with a uniform prior, calculate the posterior and plot the results
```{r}
library(pacman)
p_load(rethinking, tidyverse)

setwd("~/Desktop/OneDrive/Cognitive Science/4 semester/Computational Models")

#defining density (spacing of the sequence from 0-1)
dense = 200
#define grid
p_grid <- seq(0,1, length.out = dense)
#define prior
prior <- rep(1,dense) #flat prior
#prior <- ifelse(p_grid <0.5, 0, 1)
#prior <- exp(-5*abs(p_grid-0.5))
#compute likelihood at each value in grid
likelihood  <- dbinom(3, size = 6, prob=p_grid)
#compute product of likelihood and prior
unstd.posterior <- likelihood * prior
#standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

#turning into df
Data = data.frame(grid = p_grid, posterior=posterior, likelihood = likelihood, prior=prior)
#plotting
ggplot(Data,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/dense),color='red')+  
  xlab("probability of water")+ 
  ylab("posterior probability") 

##Not necesarry for this task..
#getting samples from grid
#samples <- sample(p_grid , prob=posterior , size=1e4 , replace=TRUE )
#plot(samples)
#dens(samples)
#sum( samples < 0.5 ) / 1e4
#sum( samples > 0.5 & samples < 0.75 ) / 1e4
#quantile( samples , 0.8 )

#add up posterior probability where p < 0.5 
sum(posterior[p_grid > 0.5])
#50% probability of Riccardo knowing more than chance,
```


- Then implement a quadratic approximation (hint check paragraph 2.4.2!).
```{r}
#quadratic approximation
riccardo.qa <- rethinking::map(
    alist(
        w ~ dbinom(6,p) ,  # binomial likelihood
        p ~ dunif(0,1)     # uniform prior
    ) ,
    data=list(w=3) )

# display summary of quadratic approximation
precis(riccardo.qa)
plot(riccardo.qa) #plot won't work :(
# analytical calculation (not really necessary..)
w <- 3
n <- 6

#plotting analytical calculation
curve( dbeta( x , w+1 , n-w+1 ) , from=0 , to=1 )
#adding the quadratic approximation
curve( dnorm( x , 0.5 , 0.2 ) , lty=2 , add=TRUE )

```

- N.B. for the rest of the exercise just keep using the grid approximation (we'll move to quadratic approximations in two classes)


2. Estimate all the teachers' knowledge of CogSci. Who's best? Use grid approximation. Comment on the posteriors of Riccardo and Mikkel.
2a. Produce plots of the prior, and posterior for each teacher.

#function to calculate posterior given number of hits, n of possibilities and size of grid
```{r}
post = function(density, p_grid, right, questions, prior){
#compute likelihood at each value in grid
likelihood  <- dbinom(right, size = questions, prob=p_grid)
#compute product of likelihood and prior
unstd.posterior <- likelihood * prior
#standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

#return values in a dataframe
return(Data = data.frame(grid = p_grid, posterior=posterior, likelihood = likelihood, prior=prior))
}
```

#Data frames and plots
```{r}
dense = 1000
#define grid
p_grid <- seq(0,1, length.out = dense)
flatPrior <- rep(1,dense) #flat prior
#prior <- ifelse(p_grid <0.5, 0, 1)
#prior <- exp(-5*abs(p_grid-0.5))


riccardo = post(dense, p_grid, 3, 6, flatPrior)
ggplot(riccardo,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(riccardo)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

kristian = post(dense, p_grid, 2, 2, flatPrior)
ggplot(kristian,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(kristian)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

josh = post(dense, p_grid, 160, 198, flatPrior)
ggplot(josh,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(josh)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

mikkel = post(dense, p_grid, 66, 132, flatPrior)
ggplot(mikkel,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(mikkel)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 
```

#Sampling and HPDI
```{r}
#Riccardo
RiccardoSamples <- sample(riccardo$grid , prob=riccardo$posterior , size=1e5 , replace=TRUE )
dens(RiccardoSamples)
HPDI(RiccardoSamples, prob = 0.5)
#the highest posterior density interval for Riccardo is from 0.36 to 0.60  cog sci knowledge (50 % interval)

#Kristian
KristianSamples <- sample(kristian$grid , prob=kristian$posterior , size=1e5 , replace=TRUE )
dens(KristianSamples)
HPDI(KristianSamples, prob = 0.5)
#the highest posterior density interval for Krisitan is from 0.79 to 1  cog sci knowledge (50 % interval)

#Josh
JoshSamples <- sample(josh$grid , prob=josh$posterior , size=1e5 , replace=TRUE )
dens(JoshSamples)
HPDI(JoshSamples, prob = 0.5)
#the highest posterior density interval for Josh is from 0.78 to 0.82  cog sci knowledge (50 % interval)

#Mikkel
MikkelSamples <- sample(mikkel$grid , prob=mikkel$posterior , size=1e5 , replace=TRUE )
dens(MikkelSamples)
HPDI(MikkelSamples, prob = 0.5)
#the highest posterior density interval for Mikkel is from 0.47 to 0.52  cog sci knowledge (50 % interval)


```
Riccardo and Mikkel both have maximum a posteriori (MAP) values of 50 %, however there is much more uncertainty regarding Riccardo's estimate. The 50% most likely values for Riccardo's cog sci knowledge (HPDI) is between 0.36 and 0.60. For Mikkel, these values lie between 0.47 and 0.52.

The 'best' teacher is Kristian. His MAP value is 1.


3. Change the prior. Given your teachers have all CogSci jobs, you should start with a higher appreciation of their knowledge: the prior is a normal distribution with a mean of 0.8 and a standard deviation of 0.2. Do the results change (and if so how)?
3a. Produce plots of the prior and posterior for each teacher.
```{r}
#set density
dense = 1000
#define grid
p_grid <- seq(0,1, length.out = dense)
#making a normally distributed prior with mean 0.8 and sd 0.2
normPrior <- dnorm(p_grid, 0.8, 0.2)
plot(normPrior)

#calculating posterior for Riccardo 
riccardo = post(dense, p_grid, 3, 6, normPrior)
#plotting
ggplot(riccardo,aes(grid,posterior))+ 
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid, prior/nrow(riccardo)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

#for kristian
kristian = post(dense, p_grid, 2, 2, normPrior)

ggplot(kristian,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(kristian)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

josh = post(dense, p_grid, 160, 198, normPrior)
ggplot(josh,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(josh)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

mikkel = post(dense, p_grid, 66, 132, normPrior)
ggplot(mikkel,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(mikkel)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 
```
#Sampling and HPDI
```{r}
#Riccardo
RiccardoSamples <- sample(riccardo$grid , prob=riccardo$posterior , size=1e5 , replace=TRUE )
dens(RiccardoSamples)
HPDI(RiccardoSamples, prob = 0.5)
#the highest posterior density interval for Riccardo is from 0.56 to 0.73  cog sci knowledge (50 % interval)

#Kristian
KristianSamples <- sample(kristian$grid , prob=kristian$posterior , size=1e5 , replace=TRUE )
dens(KristianSamples)
HPDI(KristianSamples, prob = 0.5)
#the highest posterior density interval for Krisitan is from 0.79 to 0.96  cog sci knowledge (50 % interval)

#Josh
JoshSamples <- sample(josh$grid , prob=josh$posterior , size=1e5 , replace=TRUE )
dens(JoshSamples)
HPDI(JoshSamples, prob = 0.5)
#the highest posterior density interval for Josh is from 0.78 to 0.82  cog sci knowledge (50 % interval) (unchanged)

#Mikkel
MikkelSamples <- sample(mikkel$grid , prob=mikkel$posterior , size=1e5 , replace=TRUE )
dens(MikkelSamples)
HPDI(MikkelSamples, prob = 0.5)
#the highest posterior density interval for Mikkel is from 0.48 to 0.54  cog sci knowledge (50 % interval) (largely unchanged)

```

In cases where there is a lot of uncertainty (e.g. Riccardo and Kristian due to low sample size), the posterior is pulled towards the prior quite a lot. In cases where there is low uncertainty, there is not much of a change 

4. You go back to your teachers and collect more data (multiply the previous numbers by 100). Calculate their knowledge with both a uniform prior and a normal prior with a mean of 0.8 and a standard deviation of 0.2. Do you still see a difference between the results? Why?
```{r}
dense = 1000
p_grid <- seq(0,1, length.out = dense)
#Riccardo
riccardoF = post(dense, p_grid, 300, 600, flatPrior)
riccardoN = post(dense, p_grid, 300, 600, normPrior)

ggplot(riccardoF,aes(grid,posterior))+ 
  geom_line()+
  #geom_smooth() +
  theme_classic()+ 
  geom_line(aes(grid, prior/nrow(riccardoF)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

ggplot(riccardoN,aes(grid,posterior))+ 
  geom_line()+
  #geom_smooth() +
  theme_classic()+ 
  geom_line(aes(grid, prior/nrow(riccardoN)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 


#Kristian
kristianF = post(dense, p_grid, 200, 200, flatPrior)
kristianN = post(dense, p_grid, 200, 200, normPrior)


ggplot(kristianF,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(kristianF)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

ggplot(kristianN,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(kristianN)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 


#Josh
joshF = post(dense, p_grid, 16000, 19800, flatPrior)
joshN = post(dense, p_grid, 16000, 19800, normPrior)

ggplot(joshF,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(josh)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

ggplot(joshN,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(josh)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 


mikkelF = post(dense, p_grid, 6600, 13200, flatPrior)
mikkelN = post(dense, p_grid, 6600, 13200, normPrior)

ggplot(mikkelF,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(mikkel)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

ggplot(mikkelN,aes(grid,posterior))+ 
  geom_point()+
  geom_line()+
  theme_classic()+ 
  geom_line(aes(grid,prior/nrow(mikkel)),color='red')+  xlab("cog sci knowledge")+ 
  ylab("posterior probability") 

```
#Sampling and HPDI
```{r}
#quick function to ease the sampling
samplez <- function(df){
  sample(df$grid, prob = df$posterior, size=1e5, replace=TRUE)
}
#Riccardo
ricF = samplez(riccardoF)
ricN = samplez(riccardoN)


dens(ricF)
HPDI(ricF, prob = 0.5)

dens(ricN)
HPDI(ricN, prob = 0.5)
#the highest posterior density interval for Riccardo with flat prior is from 0.48 to 0.51  cog sci knowledge (50 % interval). With norm prior it is from 0.49 to 0.51. Practically the same

#Kristian
kriF = samplez(kristianF)
kriN = samplez(kristianN)

dens(kriF)
HPDI(kriF, prob = 0.5)

dens(kriN)
HPDI(kriN, prob = 0.5)
#the highest posterior density interval for Krisitan is from 1 to 1 cog sci knowledge (50 % interval) with both priors

#Josh
josF = samplez(joshF)
josN = samplez(joshN)

dens(josF)
HPDI(josF, prob = 0.5)

dens(josN)
HPDI(josN, prob = 0.5)
#the highest posterior density interval for Josh is from 0.81 to 0.81  cog sci knowledge (50 % interval) with both priors

#Mikkel
mikF = samplez(mikkelF)
mikN = samplez(mikkelN)

dens(mikF)
HPDI(mikF, prob = 0.5)

dens(mikN)
HPDI(mikN, prob = 0.5)
#the highest posterior density interval for Mikkel is from 0.50 to 0.50  cog sci knowledge (50 % interval) with both priors

```
The larger the sample size, the less of an effect the prior has.

5. Imagine you're a skeptic and think your teachers do not know anything about CogSci, given the content of their classes. How would you operationalize that belief?

By creating a prior with a chance mean (ie. 0.5) and very low sd.

```{r}

```

6. Optional question: Can you estimate the difference between Riccardo's estimated knowledge and that of each of the other teachers? Would you deem it credible (that is, would you believe that it is actually different)?

7. Bonus knowledge: all the stuff we have done can be implemented in a lme4-like fashion using the brms package. Here is an example.
```{r}
library(brms)
d <- data.frame(
  Correct=c(3,2,160,66),
  Questions=c(6,2,198,132),
  Teacher=c("RF","KT","JS","MW"))

FlatModel <- brm(Correct|trials(Questions)~1,data=subset(d,Teacher=="RF"),family='binomial',prior=prior("uniform(0,1)", class = "Intercept"))
plot(FlatModel)
PositiveModel <- brm(Correct|trials(Questions)~1,data=subset(d,Teacher=="RF"),family ='binomial',prior=prior("normal(0.8,0.2)", class = "Intercept"))
plot(PositiveModel)
SkepticalModel <- brm(Correct|trials(Questions)~1,data=subset(d,Teacher=="RF"),family='binomial',prior=prior("normal(0.5,0.01)", class = "Intercept"))
plot(SkepticalModel)
```

If you dare, try to tweak the data and model to test two hypotheses:
- Is Kristian different from Josh?
- Is Josh different from chance?



